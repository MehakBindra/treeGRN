{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b520de87-a982-448a-b1bc-bf48dae5a841",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve, auc\n",
    "from statistics import mean\n",
    "import shap\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from scipy.stats import ranksums\n",
    "from sklearn.inspection import permutation_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1062139a-3627-4048-8175-872981d25913",
   "metadata": {},
   "outputs": [],
   "source": [
    "MR = [5, 40, 100]\n",
    "N_genes = 100  # total no. of genes\n",
    "N_TFs = N_genes\n",
    "\n",
    "n_estimators=[500, 1000, 2000]  # number of trees in the forest\n",
    "criterion='squared_error'  # variance reduction equivalent\n",
    "max_features = ['sqrt', N_TFs ] # max no. of features to use in each split\n",
    "random_state = 42  # for reproducibility\n",
    "bootstrap = [True, False ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b74dd8-845a-4c15-b171-5cd1ac80abc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_results = pd.DataFrame(columns=['MR', 'FI', 'N_EST', 'MAX_FEATURES','BOOTSTRAPPING', 'AUPRC', 'AUROC', 'MEAN_AUROC','STD_AUROC','MEAN_AUPRC','STD_AUPRC', 'p-value', 'BOOSTING'])\n",
    "rf_results.to_csv(\"results/grn_rf_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f7c0c0-23a3-4317-b9af-4635d7cf6cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve, auc\n",
    "from statistics import mean\n",
    "import shap\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from scipy.stats import ranksums\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "MR = [5, 40, 100]\n",
    "N_genes = 100  # total no. of genes\n",
    "N_TFs = N_genes\n",
    "\n",
    "n_estimators=[500, 1000, 2000]  # number of trees in the forest\n",
    "criterion='squared_error'  # variance reduction equivalent\n",
    "max_features = ['sqrt', N_TFs ] # max no. of features to use in each split\n",
    "random_state = 42  # for reproducibility\n",
    "bootstrap = [True, False ]\n",
    "\n",
    "rf_results = pd.DataFrame(columns=['MR', 'FI', 'N_EST', 'MAX_FEATURES','BOOTSTRAPPING', 'AUPRC', 'AUROC', 'MEAN_AUROC','STD_AUROC','MEAN_AUPRC','STD_AUPRC', 'p-value', 'BOOSTING'])\n",
    "rf_results.to_csv(\"results/grn_rf_results.csv\", index=False)\n",
    "\n",
    "for mr in MR:\n",
    "    for n_est in n_estimators:\n",
    "        for m_f in max_features:\n",
    "            for b_s in bootstrap:\n",
    "\n",
    "                rf_results = pd.read_csv(\"results/grn_rf_results.csv\", header=0)\n",
    "\n",
    "                data_file = (\"data/{}_mr_50_cond/simulated_noNoise.txt\").format(mr)\n",
    "                grn_file = (\"data/{}_mr_50_cond/bipartite_GRN.csv\").format(mr)\n",
    "\n",
    "                data = pd.read_csv(data_file, sep=\"\\t\", header=0)\n",
    "                grn_df = pd.read_csv(grn_file, sep = \",\", header = None, names=['TF_ID', 'G_ID'])\n",
    "                grn_df['class'] = 1\n",
    "\n",
    "                # # Normalize Expression data to unit-variance\n",
    "                # data_n = StandardScaler(with_mean=False).fit_transform(data.to_numpy())\n",
    "\n",
    "                grn_eval = pd.read_csv(\"results/{}_mr_50_cond/grn_eval_rf_vr_{}_{}_{}.csv\".format(mr,n_est,m_f,b_s))\n",
    "\n",
    "                # # # normalize weights\n",
    "                # grn_eval['W_pred'] = grn_eval['W_pred']/grn_eval['W_pred'].max()\n",
    "\n",
    "                precision, recall, thresholds_prc = precision_recall_curve(grn_eval['class'], grn_eval['W_pred'])\n",
    "                fpr, tpr, thresholds_roc = roc_curve(grn_eval['class'], grn_eval['W_pred'])\n",
    "                auprc = auc(recall, precision)\n",
    "                auroc = auc(fpr,tpr)\n",
    "\n",
    "                roc_gene = []\n",
    "                ap_gene = []\n",
    "                for i in range(100):\n",
    "                    grn_eval_gene = grn_eval.iloc[i::N_TFs,:]\n",
    "                    roc_gene.append(metrics.roc_auc_score(grn_eval_gene['class'], grn_eval_gene['W_pred']))\n",
    "                    ap_gene.append(metrics.average_precision_score(grn_eval_gene['class'], grn_eval_gene['W_pred']))\n",
    "\n",
    "                mean_auroc = mean(roc_gene)\n",
    "                sd_auroc = np.std(roc_gene)\n",
    "\n",
    "                mean_auprc = mean(ap_gene)\n",
    "                sd_auprc = np.std(ap_gene)\n",
    "\n",
    "                ranksums_pvalue = ranksums(grn_eval[grn_eval['class']==1]['W_pred'], grn_eval[grn_eval['class']==0]['W_pred'], alternative='greater')\n",
    "\n",
    "                prc = pd.DataFrame({'precision': precision, 'recall': recall}, columns=['precision', 'recall'])\n",
    "                roc = pd.DataFrame({'fpr': fpr, 'tpr': tpr}, columns=['fpr', 'tpr'])\n",
    "                prc.to_csv(\"results/{}_mr_50_cond/grn_prc_rf_vr_{}_{}_{}.csv\".format(mr,n_est,m_f,b_s))\n",
    "                roc.to_csv(\"results/{}_mr_50_cond/grn_roc_rf_vr_{}_{}_{}.csv\".format(mr,n_est,m_f,b_s))\n",
    "\n",
    "                temp1 = pd.DataFrame([[mr, \"VR\", n_est, m_f, b_s, auprc, auroc, mean_auroc,sd_auroc, mean_auprc,sd_auprc,ranksums_pvalue[1], \"No\"]], \\\n",
    "                        columns=['MR', 'FI', 'N_EST', 'MAX_FEATURES','BOOTSTRAPPING', 'AUPRC', 'AUROC', 'MEAN_AUROC','STD_AUROC','MEAN_AUPRC','STD_AUPRC', 'p-value', 'BOOSTING'])\n",
    "\n",
    "                grn_eval_shap = pd.read_csv(\"results/{}_mr_50_cond/grn_eval_rf_shap_{}_{}_{}.csv\".format(mr,n_est,m_f,b_s))\n",
    "\n",
    "                # normalize weights\n",
    "                # grn_eval_shap['W_pred'] = 1/grn_eval_shap['W_pred'].max()\n",
    "\n",
    "                precision, recall, thresholds_prc = precision_recall_curve(grn_eval_shap['class'], grn_eval_shap['W_pred'])\n",
    "                fpr, tpr, thresholds_roc = roc_curve(grn_eval_shap['class'], grn_eval_shap['W_pred'])\n",
    "                auprc = auc(recall, precision)\n",
    "                auroc = auc(fpr,tpr)\n",
    "\n",
    "                roc_gene = []\n",
    "                ap_gene = []\n",
    "                for i in range(100):\n",
    "                    grn_eval_gene = grn_eval_shap.iloc[i::N_TFs,:]\n",
    "                    roc_gene.append(metrics.roc_auc_score(grn_eval_gene['class'], grn_eval_gene['W_pred']))\n",
    "                    ap_gene.append(metrics.average_precision_score(grn_eval_gene['class'], grn_eval_gene['W_pred']))\n",
    "\n",
    "                mean_auroc = mean(roc_gene)\n",
    "                sd_auroc = np.std(roc_gene)\n",
    "\n",
    "                mean_auprc = mean(ap_gene)\n",
    "                sd_auprc = np.std(ap_gene)\n",
    "\n",
    "                ranksums_pvalue = ranksums(grn_eval_shap[grn_eval_shap['class']==1]['W_pred'], grn_eval_shap[grn_eval_shap['class']==0]['W_pred'], alternative='greater')\n",
    "\n",
    "                prc = pd.DataFrame({'precision': precision, 'recall': recall}, columns=['precision', 'recall'])\n",
    "                roc = pd.DataFrame({'fpr': fpr, 'tpr': tpr}, columns=['fpr', 'tpr'])\n",
    "                prc.to_csv(\"results/{}_mr_50_cond/grn_prc_rf_shap_{}_{}_{}.csv\".format(mr,n_est,m_f,b_s))\n",
    "                roc.to_csv(\"results/{}_mr_50_cond/grn_roc_rf_shap_{}_{}_{}.csv\".format(mr,n_est,m_f,b_s))\n",
    "\n",
    "                temp2 = pd.DataFrame([[mr, \"SHAP\", n_est, m_f, b_s, auprc, auroc, mean_auroc,sd_auroc, mean_auprc,sd_auprc,ranksums_pvalue[1], \"No\"]], \\\n",
    "                        columns=['MR', 'FI', 'N_EST', 'MAX_FEATURES','BOOTSTRAPPING', 'AUPRC', 'AUROC', 'MEAN_AUROC','STD_AUROC','MEAN_AUPRC','STD_AUPRC', 'p-value', 'BOOSTING'])\n",
    "\n",
    "                df_list = [rf_results, temp1, temp2]\n",
    "                df_out = pd.concat([df for df in df_list if not df.empty])\n",
    "                df_out.to_csv(\"results/grn_rf_results.csv\", index=False)\n",
    "\n",
    " \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "treeGRN",
   "language": "python",
   "name": "treegrn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
