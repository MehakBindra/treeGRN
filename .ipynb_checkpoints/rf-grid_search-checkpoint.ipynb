{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7d358ab-4376-49f0-aa5c-3713f1099365",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve, auc, confusion_matrix\n",
    "from sklearn.metrics import RocCurveDisplay, PrecisionRecallDisplay\n",
    "from statistics import mean, median\n",
    "import shap\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import math\n",
    "from scipy.stats import ranksums\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30e78a4e-c841-4a4b-8df5-85997a5dd0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "MR = 5\n",
    "\n",
    "data_file = (\"data/{}_mr_50_cond/simulated_noNoise.txt\").format(MR)\n",
    "grn_file = (\"data/{}_mr_50_cond/bipartite_GRN.csv\").format(MR)\n",
    "\n",
    "data = pd.read_csv(data_file, sep=\"\\t\", header=0)\n",
    "grn_df = pd.read_csv(grn_file, sep = \",\", header = None, names=['TF_ID', 'G_ID'])\n",
    "grn_df['class'] = 1\n",
    "\n",
    "N_genes = 100  # total no. of genes\n",
    "N_TFs = N_genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2f39b91b-e73a-4296-bd66-e269a864c30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators=[50, 200, 500, 1000, 2000]  # number of trees in the forest\n",
    "criterion='squared_error'  # variance reduction equivalent\n",
    "max_features = ['log2', 'sqrt', 100] # max no. of features to use in each tree\n",
    "bootstrap = [True, False]\n",
    "random_state = 42  # for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1bd7d9f4-1733-4361-b257-286e0770e571",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'bootstrap': bootstrap,\n",
    "    'max_features': max_features,\n",
    "    'n_estimators': n_estimators\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1a5cbf81-bec1-4165-acd0-19ae50e1e381",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "float() argument must be a string or a real number, not 'dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(estimator \u001b[38;5;241m=\u001b[39m M_rf, param_grid \u001b[38;5;241m=\u001b[39m param_grid, cv \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m     18\u001b[0m grid_search\u001b[38;5;241m.\u001b[39mfit(X,Gj)\n\u001b[0;32m---> 19\u001b[0m \u001b[43mbest_params\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m grid_search\u001b[38;5;241m.\u001b[39mbest_params_\n\u001b[1;32m     21\u001b[0m best_grid \u001b[38;5;241m=\u001b[39m grid_search\u001b[38;5;241m.\u001b[39mbest_estimator_\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# train score\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: float() argument must be a string or a real number, not 'dict'"
     ]
    }
   ],
   "source": [
    "# Normalize Expression data to unit-variance\n",
    "data_n = StandardScaler(with_mean=False).fit_transform(data.to_numpy())\n",
    "\n",
    "# Initialize matrices\n",
    "W = np.zeros(shape=(N_genes,N_TFs))\n",
    "W_shap = np.zeros(shape=(N_genes,N_TFs))\n",
    "Fscores = np.zeros(shape=(N_genes,))\n",
    "best_params = shape=(N_genes,3\n",
    "\n",
    "for j in np.arange(0,N_genes):\n",
    "    # read TF and gene expression data X and Gj\n",
    "    X, Gj= data_n[:,:N_TFs], data_n[:,N_genes+j]\n",
    "    \n",
    "    # fit an RF model to predict gene expression from TF\n",
    "    M_rf = RandomForestRegressor(criterion=criterion, random_state=random_state)\n",
    "    grid_search = GridSearchCV(estimator = M_rf, param_grid = param_grid, cv = 3)\n",
    "    \n",
    "    grid_search.fit(X,Gj)\n",
    "                     \n",
    "    best_grid = grid_search.best_estimator_\n",
    "    \n",
    "    # train score\n",
    "    Fscores[j] = best_grid.score(X,Gj)\n",
    "\n",
    "    # Get the weights for all edges connecting TFs to gene j\n",
    "    W[j,:] = best_grid.feature_importances_\n",
    "\n",
    "    # look at feature importance based on SHAP values\n",
    "    explainer = shap.TreeExplainer(best_grid)\n",
    "    shap_values = explainer(X)\n",
    "    W_shap[j,:] = np.mean(np.abs(shap_values.values), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1954d6b0-b14c-495a-930e-f35857c9787a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': False, 'max_features': 'log2', 'n_estimators': 50}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87cbc27-5bf7-4346-90ac-0174678c0c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_df = pd.DataFrame(np.abs(W))\n",
    "\n",
    "plt.figure(figsize=(13, 3))\n",
    "ax = sns.heatmap(W_df,\n",
    "            cmap = \"flare\",\n",
    "            linewidths=0.5)\n",
    "\n",
    "grn_pred = pd.melt(W_df.reset_index(), id_vars = 'index', var_name='TF_ID', value_name='W_pred').rename(columns={'index': 'G_ID'})\n",
    "\n",
    "grn_pred['G_ID'] = grn_pred['G_ID'].astype(np.int64) + 100\n",
    "grn_pred['TF_ID'] = grn_pred['TF_ID'].astype(np.int64)\n",
    "\n",
    "grn_eval = pd.merge(grn_pred,grn_df, on=['G_ID', 'TF_ID'], how='left')\n",
    "grn_eval['class'] = grn_eval['class'].fillna(int(0))\n",
    "\n",
    "grn_eval.to_csv(\"results/{}_mr_50_cond/grn_eval_rf_vr_{}.csv\".format(MR, max_features))\n",
    "\n",
    "precision, recall, thresholds_prc = precision_recall_curve(grn_eval['class'], grn_eval['W_pred'])\n",
    "fpr, tpr, thresholds_roc = roc_curve(grn_eval['class'], grn_eval['W_pred'])\n",
    "# Use AUC function to calculate the area under the curve of precision recall curve\n",
    "print(\"auprc\" , auc(recall, precision))\n",
    "print(\"auroc\" , auc(fpr,tpr))\n",
    "\n",
    "roc_gene = [] \n",
    "for i in range(100):\n",
    "    grn_eval_gene = grn_eval.iloc[i::N_TFs,:]\n",
    "    roc_gene.append(metrics.roc_auc_score(grn_eval_gene['class'], grn_eval_gene['W_pred']))\n",
    "    \n",
    "print(\"mean auroc\", mean(roc_gene))\n",
    "\n",
    "pr_display = PrecisionRecallDisplay(precision=precision, recall=recall).plot()\n",
    "roc_display = RocCurveDisplay(fpr=fpr, tpr=tpr).plot()\n",
    "\n",
    "print(\"ranksums\", ranksums(grn_eval[grn_eval['class']==1]['W_pred'], grn_eval[grn_eval['class']==0]['W_pred'], alternative='greater'))\n",
    "\n",
    "prc = pd.DataFrame({'precision': precision, 'recall': recall}, columns=['precision', 'recall'])\n",
    "roc = pd.DataFrame({'fpr': fpr, 'tpr': tpr}, columns=['fpr', 'tpr'])\n",
    "prc.to_csv(\"results/{}_mr_50_cond/grn_prc_rf_vr_{}.csv\".format(MR, max_features))\n",
    "roc.to_csv(\"results/{}_mr_50_cond/grn_roc_rf_vr_{}.csv\".format(MR, max_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149db51d-450f-4b2a-b1a2-d5feafcb9181",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_shap_df = pd.DataFrame(np.abs(W_shap))\n",
    "\n",
    "plt.figure(figsize=(13, 3))\n",
    "ax = sns.heatmap(W_shap_df,\n",
    "            cmap = \"flare\",\n",
    "            linewidths=0.5)\n",
    "\n",
    "grn_pred_shap = pd.melt(W_shap_df.reset_index(), id_vars = 'index', var_name='TF_ID', value_name='W_pred').rename(columns={'index': 'G_ID'})\n",
    "\n",
    "grn_pred_shap['G_ID'] = grn_pred_shap['G_ID'].astype(np.int64) + 100\n",
    "grn_pred_shap['TF_ID'] = grn_pred_shap['TF_ID'].astype(np.int64)\n",
    "\n",
    "grn_eval_shap = pd.merge(grn_pred_shap,grn_df, on=['G_ID', 'TF_ID'], how='left')\n",
    "grn_eval_shap['class'] = grn_eval_shap['class'].fillna(int(0)) == 1.0\n",
    "\n",
    "grn_eval_shap.to_csv(\"results/{}_mr_50_cond/grn_eval_rf_shap_{}.csv\".format(MR, max_features))\n",
    "\n",
    "precision, recall, thresholds_prc = precision_recall_curve(grn_eval_shap['class'], grn_eval_shap['W_pred'])\n",
    "fpr, tpr, thresholds_roc = roc_curve(grn_eval_shap['class'], grn_eval_shap['W_pred'])\n",
    "# Use AUC function to calculate the area under the curve of precision recall curve\n",
    "print(\"auprc\" , auc(recall, precision))\n",
    "print(\"auroc\" , auc(fpr,tpr))\n",
    "\n",
    "roc_gene = [] \n",
    "for i in range(100):\n",
    "    grn_eval_gene = grn_eval_shap.iloc[i::N_TFs,:]\n",
    "    roc_gene.append(metrics.roc_auc_score(grn_eval_gene['class'], grn_eval_gene['W_pred']))\n",
    "    \n",
    "print(\"mean auroc\", mean(roc_gene))\n",
    "\n",
    "pr_display = PrecisionRecallDisplay(precision=precision, recall=recall).plot()\n",
    "roc_display = RocCurveDisplay(fpr=fpr, tpr=tpr).plot()\n",
    "\n",
    "print(\"ranksums\",ranksums(grn_eval_shap[grn_eval_shap['class']==1]['W_pred'], grn_eval_shap[grn_eval_shap['class']==0]['W_pred'], alternative='greater'))\n",
    "\n",
    "prc = pd.DataFrame({'precision': precision, 'recall': recall}, columns=['precision', 'recall'])\n",
    "roc = pd.DataFrame({'fpr': fpr, 'tpr': tpr}, columns=['fpr', 'tpr'])\n",
    "prc.to_csv(\"results/{}_mr_50_cond/grn_prc_rf_shap_{}.csv\".format(MR, max_features))\n",
    "roc.to_csv(\"results/{}_mr_50_cond/grn_roc_rf_shap_{}.csv\".format(MR, max_features))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "treeGRN",
   "language": "python",
   "name": "treegrn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
