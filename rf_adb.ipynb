{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve, auc\n",
    "from statistics import mean\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "import shap\n",
    "from scipy.stats import ranksums"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "MR = [5 , 40, 100]\n",
    "N_genes = 100  # total no. of genes\n",
    "N_TFs = N_genes\n",
    "\n",
    "n_estimators=[500, 1000, 2000]  # number of trees in the forest\n",
    "criterion='squared_error'  # variance reduction equivalent\n",
    "max_features = ['sqrt', N_TFs ] # max no. of features to use in each split \n",
    "random_state = 42  # for reproducibility"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "rf_results = pd.DataFrame(columns=['MR', 'FI', 'N_EST', 'MAX_FEATURES','BOOTSTRAPPING', 'AUPRC', 'AUROC', 'MEAN_AUROC','STD_AUROC','MEAN_AUPRC','STD_AUPRC', 'p-value', 'BOOSTING'])\n",
    "rf_results.to_csv(\"results/grn_rf_ada_results.csv\", index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[8], line 28\u001B[0m\n\u001B[1;32m     26\u001B[0m \u001B[38;5;66;03m# fit a boosted RF model to predict gene expression from TF\u001B[39;00m\n\u001B[1;32m     27\u001B[0m M_dt \u001B[38;5;241m=\u001B[39m DecisionTreeRegressor(criterion\u001B[38;5;241m=\u001B[39mcriterion, max_features\u001B[38;5;241m=\u001B[39mm_f, random_state\u001B[38;5;241m=\u001B[39mrandom_state)\n\u001B[0;32m---> 28\u001B[0m M_rf_adb \u001B[38;5;241m=\u001B[39m \u001B[43mAdaBoostRegressor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mestimator\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mM_dt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_estimators\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn_est\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mloss\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mexponential\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrandom_state\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrandom_state\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43mGj\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     29\u001B[0m \u001B[38;5;66;03m# train score\u001B[39;00m\n\u001B[1;32m     30\u001B[0m Fscores[j] \u001B[38;5;241m=\u001B[39m M_rf_adb\u001B[38;5;241m.\u001B[39mscore(X,Gj)\n",
      "File \u001B[0;32m/opt/anaconda3/envs/treeGRN/lib/python3.10/site-packages/sklearn/base.py:1152\u001B[0m, in \u001B[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[0;34m(estimator, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1145\u001B[0m     estimator\u001B[38;5;241m.\u001B[39m_validate_params()\n\u001B[1;32m   1147\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[1;32m   1148\u001B[0m     skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[1;32m   1149\u001B[0m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[1;32m   1150\u001B[0m     )\n\u001B[1;32m   1151\u001B[0m ):\n\u001B[0;32m-> 1152\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfit_method\u001B[49m\u001B[43m(\u001B[49m\u001B[43mestimator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/treeGRN/lib/python3.10/site-packages/sklearn/ensemble/_weight_boosting.py:171\u001B[0m, in \u001B[0;36mBaseWeightBoosting.fit\u001B[0;34m(self, X, y, sample_weight)\u001B[0m\n\u001B[1;32m    168\u001B[0m sample_weight[zero_weight_mask] \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.0\u001B[39m\n\u001B[1;32m    170\u001B[0m \u001B[38;5;66;03m# Boosting step\u001B[39;00m\n\u001B[0;32m--> 171\u001B[0m sample_weight, estimator_weight, estimator_error \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_boost\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    172\u001B[0m \u001B[43m    \u001B[49m\u001B[43miboost\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrandom_state\u001B[49m\n\u001B[1;32m    173\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    175\u001B[0m \u001B[38;5;66;03m# Early termination\u001B[39;00m\n\u001B[1;32m    176\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m sample_weight \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[0;32m/opt/anaconda3/envs/treeGRN/lib/python3.10/site-packages/sklearn/ensemble/_weight_boosting.py:1168\u001B[0m, in \u001B[0;36mAdaBoostRegressor._boost\u001B[0;34m(self, iboost, X, y, sample_weight, random_state)\u001B[0m\n\u001B[1;32m   1166\u001B[0m X_ \u001B[38;5;241m=\u001B[39m _safe_indexing(X, bootstrap_idx)\n\u001B[1;32m   1167\u001B[0m y_ \u001B[38;5;241m=\u001B[39m _safe_indexing(y, bootstrap_idx)\n\u001B[0;32m-> 1168\u001B[0m \u001B[43mestimator\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1169\u001B[0m y_predict \u001B[38;5;241m=\u001B[39m estimator\u001B[38;5;241m.\u001B[39mpredict(X)\n\u001B[1;32m   1171\u001B[0m error_vect \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mabs(y_predict \u001B[38;5;241m-\u001B[39m y)\n",
      "File \u001B[0;32m/opt/anaconda3/envs/treeGRN/lib/python3.10/site-packages/sklearn/base.py:1152\u001B[0m, in \u001B[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[0;34m(estimator, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1145\u001B[0m     estimator\u001B[38;5;241m.\u001B[39m_validate_params()\n\u001B[1;32m   1147\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[1;32m   1148\u001B[0m     skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[1;32m   1149\u001B[0m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[1;32m   1150\u001B[0m     )\n\u001B[1;32m   1151\u001B[0m ):\n\u001B[0;32m-> 1152\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfit_method\u001B[49m\u001B[43m(\u001B[49m\u001B[43mestimator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "for mr in MR:\n",
    "    for n_est in n_estimators:\n",
    "        for m_f in max_features:\n",
    "                \n",
    "                rf_results = pd.read_csv(\"results/grn_rf_ada_results.csv\", header=0)\n",
    "                \n",
    "                data_file = (\"data/{}_mr_50_cond/simulated_noNoise.txt\").format(mr)\n",
    "                grn_file = (\"data/{}_mr_50_cond/bipartite_GRN.csv\").format(mr)\n",
    "\n",
    "                data = pd.read_csv(data_file, sep=\"\\t\", header=0)\n",
    "                grn_df = pd.read_csv(grn_file, sep = \",\", header = None, names=['TF_ID', 'G_ID'])\n",
    "                grn_df['class'] = 1\n",
    "\n",
    "                # Normalize Expression data to unit-variance\n",
    "                data_n = StandardScaler(with_mean=False).fit_transform(data.to_numpy())\n",
    "\n",
    "                # Initialize matrices\n",
    "                W = np.zeros(shape=(N_genes,N_TFs))\n",
    "                W_shap = np.zeros(shape=(N_genes,N_TFs))\n",
    "                Fscores = np.zeros(shape=(N_genes,))\n",
    "\n",
    "                for j in np.arange(0,N_genes):\n",
    "                    # read TF and gene expression data X and Gj\n",
    "                    X, Gj= data_n[:,:N_TFs], data_n[:,N_genes+j]\n",
    "\n",
    "                    # fit a boosted RF model to predict gene expression from TF\n",
    "                    M_dt = DecisionTreeRegressor(criterion=criterion, max_features=m_f, random_state=random_state)\n",
    "                    M_rf_adb = AdaBoostRegressor(estimator=M_dt, n_estimators=n_est, loss='exponential', random_state=random_state).fit(X,Gj)\n",
    "                    # train score\n",
    "                    Fscores[j] = M_rf_adb.score(X,Gj)\n",
    "\n",
    "                    # Get the weights for all edges connecting TFs to gene j\n",
    "                    W[j,:] = M_rf_adb.feature_importances_\n",
    "\n",
    "                    # look at feature importance based on SHAP values\n",
    "                    # explainer = shap.Explainer(M_rf_adb)\n",
    "                    # shap_values = explainer(X)\n",
    "                    # W_shap[j,:] = np.mean(np.abs(shap_values.values), axis=0)\n",
    "                    \n",
    "                W_df = pd.DataFrame(np.abs(W))\n",
    "\n",
    "                grn_pred = pd.melt(W_df.reset_index(), id_vars = 'index', var_name='TF_ID', value_name='W_pred').rename(columns={'index': 'G_ID'})\n",
    "\n",
    "                grn_pred['G_ID'] = grn_pred['G_ID'].astype(np.int64) + 100\n",
    "                grn_pred['TF_ID'] = grn_pred['TF_ID'].astype(np.int64)\n",
    "\n",
    "                grn_eval = pd.merge(grn_pred,grn_df, on=['G_ID', 'TF_ID'], how='left')\n",
    "                grn_eval['class'] = grn_eval['class'].fillna(int(0))\n",
    "\n",
    "                grn_eval.to_csv(\"results/{}_mr_50_cond/grn_eval_rf_vr_{}_{}_ada_{}.csv\".format(mr,n_est,m_f,'exponential'))\n",
    "\n",
    "                precision, recall, thresholds_prc = precision_recall_curve(grn_eval['class'], grn_eval['W_pred'])\n",
    "                fpr, tpr, thresholds_roc = roc_curve(grn_eval['class'], grn_eval['W_pred'])\n",
    "                # Use AUC function to calculate the area under the curve of precision recall curve\n",
    "                auprc = auc(recall, precision)\n",
    "                auroc = auc(fpr,tpr)\n",
    "\n",
    "                roc_gene = [] \n",
    "                ap_gene = []\n",
    "                for i in range(100):\n",
    "                    grn_eval_gene = grn_eval.iloc[i::N_TFs,:]\n",
    "                    roc_gene.append(metrics.roc_auc_score(grn_eval_gene['class'], grn_eval_gene['W_pred']))\n",
    "                    ap_gene.append(metrics.average_precision_score(grn_eval_gene['class'], grn_eval_gene['W_pred']))\n",
    "\n",
    "                mean_auroc = mean(roc_gene)\n",
    "                sd_auroc = np.std(roc_gene)\n",
    "                \n",
    "                mean_auprc = mean(ap_gene)\n",
    "                sd_auprc = np.std(ap_gene)\n",
    "\n",
    "                ranksums_pvalue = ranksums(grn_eval[grn_eval['class']==1]['W_pred'], grn_eval[grn_eval['class']==0]['W_pred'], alternative='greater')\n",
    "\n",
    "                prc = pd.DataFrame({'precision': precision, 'recall': recall}, columns=['precision', 'recall'])\n",
    "                roc = pd.DataFrame({'fpr': fpr, 'tpr': tpr}, columns=['fpr', 'tpr'])\n",
    "                prc.to_csv(\"results/{}_mr_50_cond/grn_prc_rf_vr_{}_{}_ada_{}.csv\".format(mr,n_est,m_f,'exponential'))\n",
    "                roc.to_csv(\"results/{}_mr_50_cond/grn_roc_rf_vr_{}_{}_ada_{}.csv\".format(mr,n_est,m_f,'exponential'))\n",
    "\n",
    "                b_s = False\n",
    "                temp = pd.DataFrame([[mr, \"VR\", n_est, m_f, b_s, auprc, auroc, mean_auroc,sd_auroc, mean_auprc, sd_auprc, ranksums_pvalue[1], \"No\"]], \\\n",
    "                        columns=['MR', 'FI', 'N_EST', 'MAX_FEATURES','BOOTSTRAPPING', 'AUPRC', 'AUROC', 'MEAN_AUROC','STD_AUROC','MEAN_AUPRC','STD_AUPRC', 'p-value', 'BOOSTING'])\n",
    "                \n",
    "                df_list = [rf_results, temp]\n",
    "                df_out = pd.concat([df for df in df_list if not df.empty])                \n",
    "                df_out.to_csv(\"results/grn_rf_ada_results.csv\", index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}